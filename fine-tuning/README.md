# 模型微调

模型微调（Fine-tuning）是指在预训练模型的基础上，使用特定领域的数据集对模型进行进一步训练，使其适应特定任务或场景的过程。

## 为什么需要微调

- 预训练模型通用性强但针对性弱：像 Qwen、Llama、BERT 等基础模型在大规模语料上训练，具备广泛知识，但在专业领域（如医疗、法律、金融）或特定任务（如情感分析、代码生成）上表现可能不够精准。
- 提升任务性能：通过微调，可以让模型更好地理解特定领域的术语、逻辑和表达方式，显著提升准确率、流畅度和实用性。
- 适配业务需求：企业可以将内部数据（如客服对话、产品文档）用于微调，打造专属 AI 助手。

## 微调方法

1. **全参数微调（Full Fine-tuning）**
- 更新模型所有层的参数。
- 优点：效果通常最好。
- 缺点：计算资源消耗大，容易过拟合小数据集。

2. **参数高效微调（PEFT, Parameter-Efficient Fine-Tuning）**

- LoRA（Low-Rank Adaptation）：只训练低秩矩阵，冻结原模型权重，大幅减少显存占用。
- Adapter Layers：在 Transformer 层间插入小型神经网络模块进行训练。
- Prefix Tuning / Prompt Tuning：仅优化输入端的可学习向量，不改动模型主体。

## LoRA原理

在预训练模型的权重矩阵中，不直接更新原始权重，而是通过训练两个低秩矩阵来近似权重的更新量。

### 数学推导

低秩假设 (Low-Rank Hypothesis)

LoRA 基于一个观察：模型在适应特定任务时，权重更新的内在秩（intrinsic rank）非常低。也就是说， 
$\Delta W$ 虽然维度很大，但它可以被两个小得多的矩阵 $B$ 和 $A$ 的乘积很好地近似：
$$
\Delta W \approx BA
$$

其中：$B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，$r \ll min(d,k)$（例如 r = 8 或 16，而 d,k 可能是几千）。

在 LoRA 中，前向传播的计算过程为：

$$
h = W_0x + \Delta W_0x + \frac{\alpha}{r}(BA)x
$$

$x \in R^k$ 是输入向量，$h \in R^d$ 是输出向量。$\alpha$ 是一个超参数（通常设为 r 的倍数，如 16,32 等）。

初始化策略：

- $A$ 使用随机高斯分布初始化（Random Gaussian）。
- $B$ 使用零初始化（Zero Initialization）。
- 这意味着在训练开始时，$\Delta W = 0$，所以 $W = W_0$。这保证了模型在训练初期的行为与预训练模型完全一致，不会破坏原有的知识。

参数量对比：

假设 $d = k = 4096$，秩 $r = 8$
- 全量微调参数量：$4096 \times 4096 \approx 1.67 \times 10^7$ （16.7M）
- LoRA 参数量：$(4096 \times 8) + (8 \times 4096) = 65536$（约0.065M)
- 压缩比：$\frac{16.7M}{0.065M} \approx 256 倍$


