{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3066d837-109f-435e-b620-3ceeb9de2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: bitsandbytes in /root/miniconda3/lib/python3.10/site-packages (0.49.2)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: trl in /root/miniconda3/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: pynvml in /root/miniconda3/lib/python3.10/site-packages (13.0.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from peft) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2026.2.19)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (4.6.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pynvml) (13.590.48)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (23.0.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.13.1)\n",
      "Requirement already satisfied: fsspec[http]<=2026.2.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2023.12.2)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: shellingham in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.5)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: triton==3.6.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.9.4)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /root/miniconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /root/miniconda3/lib/python3.10/site-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate) (1.4.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /root/miniconda3/lib/python3.10/site-packages (from typer-slim->transformers) (0.24.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.3)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.2.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2022.12.7)\n",
      "Requirement already satisfied: idna in /root/miniconda3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /root/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.13)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: rich>=12.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer>=0.24.0->typer-slim->transformers) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /root/miniconda3/lib/python3.10/site-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: click>=8.2.1 in /root/miniconda3/lib/python3.10/site-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip  install accelerate peft bitsandbytes transformers trl pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db24dd13-dd09-494f-88f5-3401299fb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# 访问huggingfase \n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94df15f7-bfb4-4b5a-a883-72174c4ffda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:65: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409587b8-2d36-4789-a31d-88b794a3420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 26 23:01:55 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 D      On  |   00000000:A8:00.0 Off |                  Off |\n",
      "| 30%   28C    P8             13W /  425W |       4MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21664d49-7a48-41ce-b863-8cc6d1135f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612cd80839744dc48adde91ced5de938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dadd7402fab40f78c7ae1ea9ef34bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82593cd49924e96b5547a73b5524d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据\n",
    "# https://huggingface.co/datasets/neil-code/dialogsum-test\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"./data/neil-code_dialogsum-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e682da-ffff-49cd-88d5-502418c7afe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0313f527-c8d7-4f75-a6d1-6f897afb9fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810dbfc-57cc-445d-9b2f-3faab84018a1",
   "metadata": {},
   "source": [
    "dialogue 和 summary.。⼀个作为输⼊，另外⼀个作为输出。 但我们要\r\n",
    "finetune 的是 LLM，格式是要满⾜ instruction data 的格式。 如果是传统 NLP 任务，我们实际上可以\r\n",
    "把<dialogue, summary>直接作为输⼊和输出送到模型⾥来处\n",
    "\n",
    "可以设计prompt\n",
    "\n",
    "Give the conversation, extract the main points and summarize the conversions\r\n",
    "{dialogue}\r\n",
    "{summary}理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79bcd517-86a4-4351-90a3-0441a49612fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    :param sample: input data\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Instruct: Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"Input: Please Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"Output:\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response] if part]\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb037f8-8623-411e-9919-255e7e45912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruct: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "Input: Please Summarize the below conversation.\n",
      "\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "\n",
      "Output:\n",
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt_formats(dataset['train'][0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9bdccd-338b-4a7a-b6fe-c7ebe1c43115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21692a6e-29b4-4576-9667-cbcd939f177d",
   "metadata": {},
   "source": [
    "1. compute_dtype = getattr(torch, \"float16\") :\r\n",
    "这⾏代码使⽤ getattr 函数从PyTorch库中获取 float16 数据类型。 float16 是16位浮点\r\n",
    "数，它具有较⼩的数值范围和精度，但⽐32位浮点数（ float32 ）占⽤更少的内存和计算资\r\n",
    "源。在量化过程中，使⽤ float16 可以减少模型的内存占⽤和加速计算。\r\n",
    "2. quant_config = BitsAndBytesConfig(...) :\r\n",
    "BitsAndBytesConfig 是 BitsAndBytes 库中⽤于配置量化参数的类。\r\n",
    "3. load_in_4bit=True :\r\n",
    "这个参数指定模型在加载时是否使⽤4位量化。设置为 True 意味着模型在加载会进⾏4位量\r\n",
    "化处理。\r\n",
    "4. bnb_4bit_quant_type=\"nf4\" :\r\n",
    "这个参数定义了4位量化的类型。 \"nf4\" 代表\"NormalFloat 4\"量，这⼀种⾮功能性 的量\r\n",
    "化⽅法，通常⽤于测试和实验，因为它不会改变模型的权重值。\r\n",
    "5. bnb_4bit_compute_dtype=compute_dtype :\r\n",
    "这个参数设置了在4位量化计算时使⽤的数值类型。这⾥使⽤了之前定义的 ompute_dtype\r\n",
    "，即 float16 。这意味着在进⾏4位量化计算时，会使⽤16位浮点数来处理\r\n",
    "数值。\r\n",
    "6. bnb_4bit_use_double_quant=True :\r\n",
    "这个参数指定是否使⽤双重量化。双重量化是⼀种技术可以在不损失太多精度的情况下进⼀\r\n",
    "步减少模型⼤⼩。双重量化是为了进⼀步量化 quantization consant，也是 Qlora ⾥⾯提出\r\n",
    "的⼀种⽅式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4f983f-625d-4804-a71b-e27308993321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f6eddf08bc40d88adf76937777ab1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模型加载\n",
    "\n",
    "model_path = \"/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct/\"\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b01132-2383-4c82-a636-699c3fffbc5a",
   "metadata": {},
   "source": [
    "model_path:这⾥我们指定了提前下载好的模型的本地路径，当然如果⽹络通畅的话，也可以将\r\n",
    "model_path替换为huggingface上的模型名字，这样可以⾃动下载到本地。\r\n",
    "device_map：⽤于定义模型参数应该放置在哪个计算卡上。字典的键是模型的参数名前缀，值是\r\n",
    "设备的编号。在这个例⼦中，键是⼀个空字符串\"\"，它通常代表所有的模型参数，⽽值 0 通常代表\r\n",
    "第⼀个可⽤的 GPU。这意味着所有模型参数都应该放在编号为 0 的 GPU 上。如果有多个 GPU，\r\n",
    "可以通过这个映射来指定不同的参数应该放在哪个 GPU 上，以实现模型的并⾏计算。\r\n",
    "quantization_config=quant_config: 使⽤ 4 位量化配置，意味着模型在加载时使⽤这量化设置，\r\n",
    "以优化内存使⽤和可能的计算效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926a7c14-81b1-41a4-bd37-1577fcbab959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 26 23:12:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 D      On  |   00000000:A8:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             13W /  425W |    6525MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39004c65-9d46-496c-a2c4-a52be25568b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True, padding_side=\n",
    "\"left\",add_eos_token=True,add_bos_token=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81f2fa-fa49-4c11-b0fa-210d44d0ee03",
   "metadata": {},
   "source": [
    "padding_side=left 这个参数指定了在序列⻓度不⾜时，在序列的哪⼀侧添加填充\r\n",
    "（padding）。在这⾥设置为\"left\"意味着填充将被添加到序列的左侧。⽐如我们在进⾏ mini-batch\r\n",
    "训练的时候，为了达到最好的训练效率，会把 mini-batch ⾥⾯的 input⻓度弄成⼀样的。 ⼀的\r\n",
    "操作是在 mini-batch 中，假如有 10 个不同的 input，⽽且每个 input ⻓度不⼀样，这时候以选\r\n",
    "择最⻓的作为标准，对于剩下的 input，⻓度不⾜的部分⽤ padding token 来填充，可以在序列的\r\n",
    "右边添加，也可以在左边添加。\r\n",
    "add_eos_token=True: 这个参数指示分词器在序列的末尾添加⼀个结束符（EOS， nd Of\r\n",
    "Sentence）。add_bos_token=True: 这个参数指示分词器在序列的开头添加⼀个开始符（BOS，\r\n",
    "Beginning Of Sentence）。 ⽐如⼀句话“I love this cat”, 添加完之后变成 <bos>I love this\r\n",
    "cat<eos> 。\r\n",
    "use_fast=False: 这个参数指定是否使⽤分词器的快速版本。设置为 False 意味着将使⽤分词器的标\r\n",
    "准版本，这通常是基于 Python 的。快速分词器通常是基于 Rust 编写的提供更好的性能和额外\r\n",
    "的功能，但在某些情况下可能不⽀持某些特殊的⾃定义⾏为。\r\n",
    "tokenizer.pad_token = tokenizer.eos_token: 这⾏代码将分词器的填充令牌（pad_token）设置为\r\n",
    "与结束符令牌（eos_token）相同。这是必要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e3b57e-6160-419f-a841-c3f9c3bf5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_path, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token_id = eval_tokenizer.eos_token_id\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bff405-5563-4594-b7fe-ebbbf6c84c38",
   "metadata": {},
   "source": [
    "toks = eval_tokenizer(p, return_tensors=\"pt\")：\n",
    "\n",
    "p: prompt,  return_tensors=”pt”指的是分词器返回的编码\n",
    "\n",
    "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "\n",
    "- **toks.to(\"cuda\")：将张量移至GPU进行加速运算\n",
    "- max_new_tokens=maxlen：生成最大token数\n",
    "- do_sample=sample：是否在生成时采样，如果为 True，则在⽣成时使⽤概率分布，这使得⽣成的\n",
    "⽂本更多样化\n",
    "- num_return_sequences=1：生成序列数量\n",
    "- temperature=0.1: 控制⽣成过程中随机性的温度参数，较低的值（如 0.1）会导致更确定性的（也\n",
    "就是说，更少随机性的）输出\n",
    "- num_beams=1: 使⽤的 beam search 的 beam 数量，设置为 1 意味着不使⽤beam search。\n",
    "- top_p=0.95: 使⽤ nucleus sampling 时保留的累积概率分布的部分。⽐如按照单词分布的⼤⼩排序\n",
    "之后：单词 1: 概率 1， 单词 2:概率 2， 单词 3:概率 3 … 如果概率 1+概率 2+…概率 20，好\r\n",
    ">=0.95, 那我们就从前 20 个单词中进⾏采样。。。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fde0740-591a-4d5b-947d-438cd9b04184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "Input:#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The conversation is about a birthday party. Person1, who is Brian, is given a gift and invited to the party. Person2 is the birthday person. They dance together and Person2 compliments Person1 on their appearance. Person1 thanks them and suggests having a drink together to celebrate the birthday. The party is described as wonderful and the atmosphere is festive.\n",
      "CPU times: user 3.76 s, sys: 374 ms, total: 4.14 s\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput:{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,200,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e38ac61-0cf7-400c-8f68-ba80bb1859cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "        if not max_length:\n",
    "            max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer( \n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035e76d5-106a-4c6f-9823-50a1731d4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset): \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "    # 过滤样本\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    # Shuffle 数据\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    print(\"Preprocessing dataset done.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2154139b-25d9-4886-8513-5fe6afa096f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default max length: 1024\n",
      "Found max lenth: 8192\n",
      "8192\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554b0ba636a0446aac9b4696eedd42b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6df7ee341c49bab8d69b18aacb5779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb87934c9d7f42ceb3771ac1615fd271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset done.\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5d81680a6d4c90a3e6be264b38c7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba7c5a04234487a522be544026fa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c410319fe7db40fb8b2643b8b274533c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset done.\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc7945a-6888-4e4e-9ef4-4b3f266dd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1999\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43803bd2-f387-4177-a7b6-d4644bd03e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型实例化\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "config = LoraConfig(\n",
    "    r=64, #Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.01, # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78f4086a-4569-4adc-8455-8dd5ee97ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "import transformers\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=2000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b6052b6-1a55-4a56-833d-f5ce29cd7fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 27:32, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.542025</td>\n",
       "      <td>1.480162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.346351</td>\n",
       "      <td>1.458464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.370595</td>\n",
       "      <td>1.447223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.360508</td>\n",
       "      <td>1.445439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.335817</td>\n",
       "      <td>1.440325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.390840</td>\n",
       "      <td>1.436420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.359330</td>\n",
       "      <td>1.434214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.306064</td>\n",
       "      <td>1.431995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.354087</td>\n",
       "      <td>1.430049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.279286</td>\n",
       "      <td>1.427958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.309579</td>\n",
       "      <td>1.425416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.306227</td>\n",
       "      <td>1.421333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.294070</td>\n",
       "      <td>1.420806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.309696</td>\n",
       "      <td>1.419609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.318951</td>\n",
       "      <td>1.416484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.242841</td>\n",
       "      <td>1.415921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.314739</td>\n",
       "      <td>1.414270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.340328</td>\n",
       "      <td>1.413325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.277202</td>\n",
       "      <td>1.412788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.291395</td>\n",
       "      <td>1.412501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=1.3324966049194336, metrics={'train_runtime': 1652.8695, 'train_samples_per_second': 1.21, 'train_steps_per_second': 1.21, 'total_flos': 2.380351562177741e+16, 'train_loss': 1.3324966049194336, 'epoch': 1.0005002501250626})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d98672ef-c384-4d98-89d8-9de4857c37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n \\\n",
    "                all model parameters: {all_model_params}\\n \\\n",
    "                percentage of trainable model parameters:  \\\n",
    "                {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0c0622f-3d07-4dac-ab74-1c90b2aee5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 54525952\\n                 all model parameters: 4595126272\\n                 percentage of trainable model parameters:                  1.19%'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b2a59fa-9326-4de2-a5d9-18b50f3dba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f8cf195-f1cb-420f-aaec-04ba49188a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 27 00:22:35 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 D      On  |   00000000:A8:00.0 Off |                  Off |\n",
      "| 30%   28C    P8             13W /  425W |   10781MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b28caf43-07bb-40eb-a4d4-8b7ed98d63c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75891087442c431eaa3c91faefe1242c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_path = \"/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct/\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a229b1fa-8757-4dbd-9634-11333f2dbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"./peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",\n",
    "    torch_dtype=torch.float16,\n",
    "    is_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9454630-6a3f-4020-a8ce-5d5aaa7e9262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 27 00:23:33 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 D      On  |   00000000:A8:00.0 Off |                  Off |\n",
      "| 30%   31C    P2             50W /  425W |   17911MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae1fe09-985a-474b-b863-b9f93924e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_path, add_bos_token=True,\n",
    "trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85e7c6c5-8419-4754-9d9e-88ae7fa5bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.5,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5715f37-7699-4176-a493-341a93f701aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "Input:#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL GENERATION:\n",
      "#Person1# gives Brian a birthday gift and they dance together. #Person2# thinks #Person1# looks great and they drink together to celebrate his birthday. #Person1# feels #Person2# is popular with everyone. #Person2# thinks #Person1# looks great and #Person1# thinks #Person2# looks pretty. #Person1# hopes #Person2# has a good time. #Person2# thinks #Person1# is popular with everyone. #Person1# and #Person2# have a good time together. #Person1# thinks #Person2# looks pretty and #Person2# thinks #Person1# looks great. #Person1# and #Person2# feel they have a good time together. #Person1# and #Person2# have a good time together. #Person1# feels #Person2# is popular with everyone. #Person2# thinks #Person1# looks great. #Person1\n",
      "CPU times: user 11.3 s, sys: 57.2 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput:{prompt}\\nOutput:\\n\"\n",
    "res = gen(ft_model,formatted_prompt,200,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL GENERATION:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef8ebf-a1db-4432-9b22-032434ef6e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
